{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis, Data Pre Processing and Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y\n",
    "df = pd.read_csv('train.csv').set_index('encounter_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y\n",
    "X_test = pd.read_csv('test.csv').set_index('encounter_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "\n",
    "    train = df.drop(columns=['readmitted_binary', 'readmitted_multiclass'])\n",
    "    target_multiclass = df['readmitted_multiclass']\n",
    "    target_binary = df['readmitted_binary']\n",
    "    return train, target_multiclass, target_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(train, X_test):\n",
    "    # Step 1: Add hospital_visits column\n",
    "    train['hospital_visits'] = train.groupby('patient_id')['patient_id'].transform('count')\n",
    "    X_test['hospital_visits'] = X_test.groupby('patient_id')['patient_id'].transform('count')\n",
    "    \n",
    "    # Step 3: Calculate Emergency_visits/total_visits ratio\n",
    "    train['Emergency_visits/total_visits'] = train['emergency_visits_in_previous_year'] / (\n",
    "        train['inpatient_visits_in_previous_year'] + train['outpatient_visits_in_previous_year'] + train['emergency_visits_in_previous_year'])\n",
    "    X_test['Emergency_visits/total_visits'] = X_test['emergency_visits_in_previous_year'] / (\n",
    "        X_test['inpatient_visits_in_previous_year'] + X_test['outpatient_visits_in_previous_year'] + X_test['emergency_visits_in_previous_year'])\n",
    "    \n",
    "    # Step 4: Calculate n_medications/length_of_stay ratio\n",
    "    train['n_medications/length_of_stay'] = train['number_of_medications'] / train['length_of_stay_in_hospital']\n",
    "    X_test['n_medications/length_of_stay'] = X_test['number_of_medications'] / X_test['length_of_stay_in_hospital']\n",
    "    \n",
    "    return train, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(train, X_test, target_binary_col='readmitted_binary', target_multiclass_col='readmitted_multiclass'):\n",
    "    # Step 1: Prepare train, target_multiclass, and target_binary\n",
    "    train = train.drop(columns=[target_binary_col, target_multiclass_col])\n",
    "    target_multiclass = train[target_multiclass_col]\n",
    "    target_binary = train[target_binary_col]\n",
    "    \n",
    "    # Step 2: Calculate hospital_visits per patient\n",
    "    train['hospital_visits'] = train.groupby('patient_id')['patient_id'].transform('count')\n",
    "    X_test['hospital_visits'] = X_test.groupby('patient_id')['patient_id'].transform('count')\n",
    "    \n",
    "    # Step 3: Calculate Emergency_visits/total_visits ratio\n",
    "    train['Emergency_visits/total_visits'] = train['emergency_visits_in_previous_year'] / (\n",
    "            train['inpatient_visits_in_previous_year'] + train['outpatient_visits_in_previous_year'] + train[\n",
    "        'emergency_visits_in_previous_year'])\n",
    "    X_test['Emergency_visits/total_visits'] = X_test['emergency_visits_in_previous_year'] / (\n",
    "            X_test['inpatient_visits_in_previous_year'] + X_test['outpatient_visits_in_previous_year'] + X_test[\n",
    "        'emergency_visits_in_previous_year'])\n",
    "    \n",
    "    # Step 4: Calculate n_medications/lenght_of_stay ratio\n",
    "    train['n_medications/lenght_of_stay'] = train['number_of_medications'] / train['length_of_stay_in_hospital']\n",
    "    X_test['n_medications/lenght_of_stay'] = X_test['number_of_medications'] / X_test['length_of_stay_in_hospital']\n",
    "    \n",
    "    return train, X_test, target_multiclass, target_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    # Strip whitespace from string values\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    \n",
    "    # Replace specific placeholder values with NaN\n",
    "    df.replace('?', np.nan, inplace=True)\n",
    "    df.replace('Unknown/Invalid', np.nan, inplace=True)\n",
    "    df.replace(['Not Mapped', 'Not Available'], np.nan, inplace=True)\n",
    "    \n",
    "    # Drop the 'country' column if it exists\n",
    "    if 'country' in df.columns:\n",
    "        df.drop('country', axis=1, inplace=True)\n",
    "    \n",
    "    # Fill NaNs in specific columns with 'Not_tested'\n",
    "    if 'glucose_test_result' in df.columns:\n",
    "        df['glucose_test_result'].fillna('Not_tested', inplace=True)\n",
    "    if 'a1c_test_result' in df.columns:\n",
    "        df['a1c_test_result'].fillna('Not_tested', inplace=True)\n",
    "    \n",
    "    # Map 'payer_code' column: NaN to 0, others to 1\n",
    "    if 'payer_code' in df.columns:\n",
    "        df['payer_code'] = df['payer_code'].map(lambda x: 0 if pd.isna(x) else 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_age_column(df):\n",
    "    def col_age(age):\n",
    "        if not pd.isna(age):\n",
    "            new_value = age.split('-')\n",
    "            age_1 = int(new_value[0].strip('['))\n",
    "            age_2 = int(new_value[1].strip(')'))\n",
    "            return (age_1 + age_2) / 2\n",
    "        else:\n",
    "            return np.nan\n",
    "    \n",
    "    if 'age' in df.columns:\n",
    "        df['age'] = df['age'].apply(col_age)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def drop_newborn_outliers(df, target_df):\n",
    "    # Find indices to drop\n",
    "    to_drop = df[(df['admission_type'] == 'Newborn') & (df['age'] > 5)].index.to_list()\n",
    "    # Drop rows in the main DataFrame\n",
    "    df.drop(to_drop, inplace=True)\n",
    "    # Drop rows in the target DataFrame\n",
    "    target_df.drop(to_drop, inplace=True)\n",
    "    return df, target_df\n",
    "\n",
    "def drop_newborn_outliers1(df):\n",
    "    # Find indices to drop\n",
    "    to_drop = df[(df['admission_type'] == 'Newborn') & (df['age'] > 5)].index.to_list()\n",
    "    # Drop rows in the main DataFrame\n",
    "    df.drop(to_drop, inplace=True)\n",
    "    return df\n",
    "\n",
    "def fill_missing_age(train, test):\n",
    "    # Fill missing 'age' values in train set\n",
    "    train['age'] = train.groupby('patient_id')['age'].transform(lambda x: x.fillna(x.mean()))\n",
    "    # Calculate mean age per patient\n",
    "    train_patient_means = train.groupby('patient_id')['age'].mean()\n",
    "    # Fill missing 'age' values in test set using train's patient means\n",
    "    test['age'] = test['patient_id'].map(train_patient_means).where(test['age'].isnull(), test['age'])\n",
    "    return train, test\n",
    "\n",
    "def fill_missing_race(train, test):\n",
    "    # Fill missing 'race' values in train set\n",
    "    train['race'] = train.groupby('patient_id')['race'].transform(lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else np.nan))\n",
    "    # Calculate mode race per patient\n",
    "    modes = train.groupby('patient_id')['race'].apply(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
    "    # Fill missing 'race' values in test set using train's patient modes\n",
    "    test['race'] = test['patient_id'].map(modes).where(test['race'].isnull(), test['race'])\n",
    "    return train, test\n",
    "\n",
    "def impute_missing_values(train, test):\n",
    "    # Impute missing values for numeric columns with mean\n",
    "    numeric = train.select_dtypes(include=np.number).columns\n",
    "    imputer_numeric = SimpleImputer(strategy='mean')\n",
    "    imputer_numeric.fit(train[numeric])\n",
    "    train[numeric] = imputer_numeric.transform(train[numeric])\n",
    "    test[numeric] = imputer_numeric.transform(test[numeric])\n",
    "    \n",
    "    # Impute missing values for categorical columns with the most frequent value\n",
    "    object_columns = train.select_dtypes(include=['object']).columns\n",
    "    imputer_object = SimpleImputer(strategy='most_frequent')\n",
    "    imputer_object.fit(train[object_columns])\n",
    "    train[object_columns] = imputer_object.transform(train[object_columns])\n",
    "    test[object_columns] = imputer_object.transform(test[object_columns])\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "def map_icd9_to_category(df):\n",
    "    def icd9_to_category(icd9):\n",
    "        if not pd.isna(icd9):\n",
    "            if icd9[0] == 'E' or icd9[0] == 'V':\n",
    "                return 'other health factors and external causes'\n",
    "            else:\n",
    "                icd9 = float(icd9)\n",
    "                if icd9 >= 1 and icd9 <= 139:\n",
    "                    return 'infectious and parasitic diseases'\n",
    "                elif icd9 >= 140 and icd9 <= 239:\n",
    "                    return 'neoplasms'\n",
    "                elif icd9 >= 240 and icd9 <= 279:\n",
    "                    return 'diabetes, endocrine and metabolic disorders'\n",
    "                elif icd9 >= 280 and icd9 <= 289:\n",
    "                    return 'other diseases/conditions'\n",
    "                elif icd9 >= 290 and icd9 <= 319:\n",
    "                    return 'mental disorders'\n",
    "                elif icd9 >= 320 and icd9 <= 389:\n",
    "                    return 'other diseases/conditions'\n",
    "                elif icd9 >= 390 and icd9 <= 459:\n",
    "                    return 'circulatory system diseases'\n",
    "                elif icd9 >= 460 and icd9 <= 519:\n",
    "                    return 'respiratory system diseases'\n",
    "                elif icd9 >= 520 and icd9 <= 579:\n",
    "                    return 'digestive system diseases'\n",
    "                elif icd9 >= 580 and icd9 <= 629:\n",
    "                    return 'genitourinary system diseases'\n",
    "                elif icd9 >= 630 and icd9 <= 679:\n",
    "                    return 'other diseases/conditions'\n",
    "                elif icd9 >= 680 and icd9 <= 709:\n",
    "                    return 'skin diseases'\n",
    "                elif icd9 >= 710 and icd9 <= 739:\n",
    "                    return 'musculoskeletal disorders'\n",
    "                elif icd9 >= 740 and icd9 <= 759:\n",
    "                    return 'other diseases/conditions'\n",
    "                elif icd9 >= 760 and icd9 <= 779:\n",
    "                    return 'perinatal conditions'\n",
    "                elif icd9 >= 780 and icd9 <= 799:\n",
    "                    return 'uncertain conditions'\n",
    "                elif icd9 >= 800 and icd9 <= 999:\n",
    "                    return 'injury and poisoning'\n",
    "                elif icd9 == 0:\n",
    "                    return 'None'\n",
    "        else:\n",
    "            return np.nan\n",
    "    \n",
    "    columns_to_map = ['primary_diagnosis', 'secondary_diagnosis', 'additional_diagnosis']\n",
    "    for col in columns_to_map:\n",
    "        df[col] = df[col].apply(icd9_to_category)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_diagnosis_columns(train, X_test):\n",
    "    # Step 1: Generate unique diagnosis values\n",
    "    diagnosis_values = train['primary_diagnosis'].unique().tolist() + \\\n",
    "                       train['secondary_diagnosis'].unique().tolist() + \\\n",
    "                       train['additional_diagnosis'].unique().tolist()\n",
    "    diagnosis_values = list(set(diagnosis_values))\n",
    "    \n",
    "    # Step 2: Create binary columns for each unique diagnosis value in train\n",
    "    for i in diagnosis_values:\n",
    "        train['diagnosis_' + i] = (train['primary_diagnosis'].str.contains(i) | \n",
    "                                    train['secondary_diagnosis'].str.contains(i) | \n",
    "                                    train['additional_diagnosis'].str.contains(i)).astype(int)\n",
    "    \n",
    "    # Drop original diagnosis columns from train\n",
    "    train.drop(['primary_diagnosis', 'secondary_diagnosis', 'additional_diagnosis'], axis=1, inplace=True)\n",
    "    \n",
    "    # Step 3: Create binary columns for each unique diagnosis value in X_test\n",
    "    for i in diagnosis_values:\n",
    "        X_test['diagnosis_' + i] = (X_test['primary_diagnosis'].str.contains(i) | \n",
    "                                    X_test['secondary_diagnosis'].str.contains(i) | \n",
    "                                    X_test['additional_diagnosis'].str.contains(i)).astype(int)\n",
    "    \n",
    "    # Drop original diagnosis columns from X_test\n",
    "    X_test.drop(['primary_diagnosis', 'secondary_diagnosis', 'additional_diagnosis'], axis=1, inplace=True)\n",
    "    \n",
    "    return train, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_categorical_to_numeric(train, X_test, target_binary=None):\n",
    "    # Mapping for 'gender'\n",
    "    train['gender'] = train['gender'].map({'Male': 0, 'Female': 1})\n",
    "    X_test['gender'] = X_test['gender'].map({'Male': 0, 'Female': 1})\n",
    "    \n",
    "    # Mapping for 'change_in_meds_during_hospitalization'\n",
    "    train['change_in_meds_during_hospitalization'] = train['change_in_meds_during_hospitalization'].map({'No': 0, 'Ch': 1})\n",
    "    X_test['change_in_meds_during_hospitalization'] = X_test['change_in_meds_during_hospitalization'].map({'No': 0, 'Ch': 1})\n",
    "    \n",
    "    # Mapping for 'prescribed_diabetes_meds'\n",
    "    train['prescribed_diabetes_meds'] = train['prescribed_diabetes_meds'].map({'No': 0, 'Yes': 1})\n",
    "    X_test['prescribed_diabetes_meds'] = X_test['prescribed_diabetes_meds'].map({'No': 0, 'Yes': 1})\n",
    "    \n",
    "    # Mapping for target_binary (if provided)\n",
    "    if target_binary is not None:\n",
    "        target_binary = target_binary.map({'No': 0, 'Yes': 1})\n",
    "    \n",
    "    return train, X_test, target_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode_admission_type(train, X_test):\n",
    "    # Define the mapping based on the specified order\n",
    "    admission_type_mapping = {'Emergency': 1, 'Urgent': 2, 'Elective': 3, 'Newborn': 4, 'Trauma Center': 5}\n",
    "    \n",
    "    # Apply the mapping to 'admission_type' column in train and X_test\n",
    "    train['admission_type'] = train['admission_type'].map(admission_type_mapping)\n",
    "    X_test['admission_type'] = X_test['admission_type'].map(admission_type_mapping)\n",
    "    \n",
    "    return train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_admission_source(train, X_test):\n",
    "    # Define the mapping function for 'admission_source' column\n",
    "    map_func = lambda x: 'Transfer from Another Health Facility' if 'Transfer' in x else x\n",
    "    \n",
    "    # Apply the mapping function to 'admission_source' column in train and X_test\n",
    "    train['admission_source'] = train['admission_source'].map(map_func)\n",
    "    X_test['admission_source'] = X_test['admission_source'].map(map_func)\n",
    "    \n",
    "    return train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_and_combine(train, X_test):\n",
    "    # Step 1: Identify categorical columns\n",
    "    obj_cols = train.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Step 2: Initialize OneHotEncoder\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    \n",
    "    # Step 3: Fit and transform on train\n",
    "    X_train_encoded_bm = encoder.fit_transform(train[obj_cols])\n",
    "    \n",
    "    # Step 4: Transform X_test\n",
    "    X_test_encoded_bm = encoder.transform(X_test[obj_cols])\n",
    "    \n",
    "    # Step 5: Get feature names after encoding\n",
    "    encoded_columns = encoder.get_feature_names_out(obj_cols)\n",
    "    \n",
    "    # Step 6: Create DataFrames for encoded features\n",
    "    X_train_encoded_bm_df = pd.DataFrame(X_train_encoded_bm, columns=encoded_columns, index=train.index)\n",
    "    X_test_encoded_bm_df = pd.DataFrame(X_test_encoded_bm, columns=encoded_columns, index=X_test.index)\n",
    "    \n",
    "    # Step 7: Drop original categorical columns from train and X_test\n",
    "    X_train_num = train.drop(obj_cols, axis=1)\n",
    "    X_test_num = X_test.drop(obj_cols, axis=1)\n",
    "    \n",
    "    # Concatenate numeric and encoded categorical DataFrames\n",
    "    train = pd.concat([X_train_num, X_train_encoded_bm_df], axis=1)\n",
    "    X_test = pd.concat([X_test_num, X_test_encoded_bm_df], axis=1)\n",
    "    \n",
    "    return train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_patient_id_column(train, X_test):\n",
    "    # Drop 'patient_id' column from train and X_test\n",
    "    train.drop('patient_id', axis=1, inplace=True)\n",
    "    X_test.drop('patient_id', axis=1, inplace=True)\n",
    "    \n",
    "    return train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_numerical_columns(train, X_test):\n",
    "    # Step 1: Identify numerical columns\n",
    "    numerical = train.select_dtypes(include=np.number).columns.tolist()\n",
    "    \n",
    "    # Step 2: Count unique values for each numerical column\n",
    "    cat = {col: train[col].nunique() for col in numerical}\n",
    "    \n",
    "    # Step 3: Categorize columns into non-binary and binary\n",
    "    non_binary = [col for col in cat if cat[col] != 2]\n",
    "    binary = [col for col in cat if cat[col] == 2]\n",
    "    \n",
    "    # Step 4: Initialize RobustScaler\n",
    "    scaler = RobustScaler()\n",
    "    \n",
    "    # Step 5: Fit and transform non-binary columns on train, and transform on X_test\n",
    "    train.loc[:, non_binary] = scaler.fit_transform(train[non_binary])\n",
    "    X_test.loc[:, non_binary] = scaler.transform(X_test[non_binary])\n",
    "    \n",
    "    return train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_with_models(train, X_test, target_binary):\n",
    "    # Step 1: Random Forest Feature Importance\n",
    "    rf_model = RandomForestClassifier(class_weight='balanced', random_state=42, max_depth=6, n_jobs=-1)\n",
    "    rf_model.fit(train, target_binary)\n",
    "    \n",
    "    feature_importances = rf_model.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({'Feature': train.columns, 'Importance': feature_importances})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "    sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "    top_30_features = sorted_indices[:30]\n",
    "    selected_features = train.columns[top_30_features]\n",
    "    \n",
    "    selected = {}\n",
    "    for i in selected_features:\n",
    "        selected[i] = 1\n",
    "    \n",
    "    # Step 2: RFE with Random Forest took more than 10 minutes to run and it always returned the following 2 features\n",
    "    selected_features = ['inpatient_visits_in_previous_year', 'hospital_visits']\n",
    "    for i in selected_features:\n",
    "        if i in selected.keys():\n",
    "            selected[i] += 1\n",
    "        else:\n",
    "            selected[i] = 1\n",
    "    \n",
    "    # Step 3: Lasso Feature Selection\n",
    "    reg = LassoCV().fit(train, target_binary)\n",
    "    coef = pd.Series(reg.coef_, index=train.columns)\n",
    "    print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "    \n",
    "    lasso_features = coef[coef != 0].index.tolist()\n",
    "    for i in lasso_features:\n",
    "        if i in selected.keys():\n",
    "            selected[i] += 1\n",
    "        else:\n",
    "            selected[i] = 1\n",
    "    \n",
    "    # Step 4: Select features to keep\n",
    "    keep = [i for i in selected.keys() if selected[i] >= 2]\n",
    "    \n",
    "    # Step 5: Subset train and X_test DataFrames\n",
    "    final_train = train[keep]\n",
    "    final_X_test = X_test[keep]\n",
    "    \n",
    "    return final_train, final_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\test_env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\José Marçal\\AppData\\Local\\Temp\\ipykernel_26692\\1348334179.py:16: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  train.loc[:, non_binary] = scaler.fit_transform(train[non_binary])\n",
      "C:\\Users\\José Marçal\\AppData\\Local\\Temp\\ipykernel_26692\\1348334179.py:17: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  X_test.loc[:, non_binary] = scaler.transform(X_test[non_binary])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso picked 61 variables and eliminated the other 397 variables\n"
     ]
    }
   ],
   "source": [
    "train, target_multiclass, target_binary = prepare_data(df)\n",
    "\n",
    "train, X_test = process_data(train, X_test)\n",
    "\n",
    "train = clean_data(train)\n",
    "X_test = clean_data(X_test)\n",
    "\n",
    "train = process_age_column(train)\n",
    "X_test = process_age_column(X_test)\n",
    "\n",
    "train, target_binary = drop_newborn_outliers(train, target_binary)\n",
    "X_test = drop_newborn_outliers1(X_test)\n",
    "\n",
    "train, X_test = fill_missing_age(train, X_test)\n",
    "\n",
    "train, X_test = fill_missing_race(train, X_test)\n",
    "\n",
    "train, X_test = impute_missing_values(train, X_test)\n",
    "\n",
    "train = map_icd9_to_category(train)\n",
    "X_test = map_icd9_to_category(X_test)\n",
    "\n",
    "train, X_test = create_diagnosis_columns(train, X_test)\n",
    "\n",
    "train, X_test,target_binary = map_categorical_to_numeric(train, X_test, target_binary)\n",
    "\n",
    "train, X_test = label_encode_admission_type(train, X_test)\n",
    "\n",
    "train, X_test = map_admission_source(train, X_test)\n",
    "\n",
    "train, X_test = one_hot_encode_and_combine(train, X_test)\n",
    "\n",
    "train, X_test = drop_patient_id_column(train, X_test)\n",
    "\n",
    "train, X_test = scale_numerical_columns(train, X_test)\n",
    "\n",
    "final_train, final_X_test = select_features_with_models(train, X_test, target_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(final_train, target_binary, test_size=0.3, random_state=1)\n",
    "X_train.rename(columns={\"medication_['insulin']\":\"medication_insulin\"}, inplace=True)\n",
    "X_val.rename(columns={\"medication_['insulin']\":\"medication_insulin\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8875058493214787\n",
      "Confusion Matrix:\n",
      "[[18874   142]\n",
      " [ 2262    92]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.99      0.94     19016\n",
      "           1       0.39      0.04      0.07      2354\n",
      "\n",
      "    accuracy                           0.89     21370\n",
      "   macro avg       0.64      0.52      0.51     21370\n",
      "weighted avg       0.84      0.89      0.84     21370\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter = 200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_lr = model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred_lr)\n",
    "conf_matrix = confusion_matrix(y_val, y_pred_lr)\n",
    "class_report = classification_report(y_val, y_pred_lr)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.8099204492278895\n",
      "Decision Tree Confusion Matrix:\n",
      "[[16796  2220]\n",
      " [ 1842   512]]\n",
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89     19016\n",
      "           1       0.19      0.22      0.20      2354\n",
      "\n",
      "    accuracy                           0.81     21370\n",
      "   macro avg       0.54      0.55      0.55     21370\n",
      "weighted avg       0.82      0.81      0.82     21370\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTreeClassifier(random_state=1)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "y_pred_dt = dt_model.predict(X_val)\n",
    "accuracy_dt = accuracy_score(y_val, y_pred_dt)\n",
    "conf_matrix_dt = confusion_matrix(y_val, y_pred_dt)\n",
    "class_report_dt = classification_report(y_val, y_pred_dt)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Decision Tree Accuracy: {accuracy_dt}\")\n",
    "print(\"Decision Tree Confusion Matrix:\")\n",
    "print(conf_matrix_dt)\n",
    "print(\"Decision Tree Classification Report:\")\n",
    "print(class_report_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
      "Best Parameters: {'bootstrap': False, 'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Best Random Forest Accuracy: 0.8891904539073467\n",
      "Best Random Forest Confusion Matrix:\n",
      "[[18952    64]\n",
      " [ 2304    50]]\n",
      "Best Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94     19016\n",
      "           1       0.44      0.02      0.04      2354\n",
      "\n",
      "    accuracy                           0.89     21370\n",
      "   macro avg       0.67      0.51      0.49     21370\n",
      "weighted avg       0.84      0.89      0.84     21370\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Instantiate the Random Forest classifier\n",
    "rf_model = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# Instantiate the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, \n",
    "                           cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Step 5: Evaluate the best model\n",
    "y_pred_best_rf = best_rf_model.predict(X_val)\n",
    "accuracy_best_rf = accuracy_score(y_val, y_pred_best_rf)\n",
    "conf_matrix_best_rf = confusion_matrix(y_val, y_pred_best_rf)\n",
    "class_report_best_rf = classification_report(y_val, y_pred_best_rf)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Random Forest Accuracy: {accuracy_best_rf}\")\n",
    "print(\"Best Random Forest Confusion Matrix:\")\n",
    "print(conf_matrix_best_rf)\n",
    "print(\"Best Random Forest Classification Report:\")\n",
    "print(class_report_best_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\test_env\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy: 0.8890500701918578\n",
      "SVC Confusion Matrix:\n",
      "[[18943    73]\n",
      " [ 2298    56]]\n",
      "SVC Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94     19016\n",
      "           1       0.43      0.02      0.05      2354\n",
      "\n",
      "    accuracy                           0.89     21370\n",
      "   macro avg       0.66      0.51      0.49     21370\n",
      "weighted avg       0.84      0.89      0.84     21370\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\test_env\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svc_model = LinearSVC(max_iter = 5000, random_state=1)\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "y_pred_svc= svc_model.predict(X_val)\n",
    "accuracy_svc = accuracy_score(y_val, y_pred_svc)\n",
    "conf_matrix_svc = confusion_matrix(y_val, y_pred_svc)\n",
    "class_report_svc = classification_report(y_val, y_pred_svc)\n",
    "\n",
    "# Print the results\n",
    "print(f\"SVC Accuracy: {accuracy_svc}\")\n",
    "print(\"SVC Confusion Matrix:\")\n",
    "print(conf_matrix_svc)\n",
    "print(\"SVC Classification Report:\")\n",
    "print(class_report_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Best XGBoost Accuracy: 0.8894244267664951\n",
      "Best XGBoost Confusion Matrix:\n",
      "[[18962    54]\n",
      " [ 2309    45]]\n",
      "Best XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94     19016\n",
      "           1       0.45      0.02      0.04      2354\n",
      "\n",
      "    accuracy                           0.89     21370\n",
      "   macro avg       0.67      0.51      0.49     21370\n",
      "weighted avg       0.84      0.89      0.84     21370\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Instantiate the XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(random_state=1, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Instantiate the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, \n",
    "                           cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Step 5: Evaluate the best model\n",
    "y_pred_best_xgb = best_xgb_model.predict(X_val)\n",
    "accuracy_best_xgb = accuracy_score(y_val, y_pred_best_xgb)\n",
    "conf_matrix_best_xgb = confusion_matrix(y_val, y_pred_best_xgb)\n",
    "class_report_best_xgb = classification_report(y_val, y_pred_best_xgb)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best XGBoost Accuracy: {accuracy_best_xgb}\")\n",
    "print(\"Best XGBoost Confusion Matrix:\")\n",
    "print(conf_matrix_best_xgb)\n",
    "print(\"Best XGBoost Classification Report:\")\n",
    "print(class_report_best_xgb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
